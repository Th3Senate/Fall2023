{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"random-variable\" style=\"color:blue\">Random variable</b>: A random variable $X$ is a function from the outcome space to the real numbers\n",
    "\n",
    "$$X:\\Omega\\rightarrow\\mathbb{R}$$\n",
    "\n",
    "The <b id=\"range\" style=\"color:blue\">range</b> of a random variable $X$, often denoted $Range(X)$ or $R_x$, is the set of possible values of $X$.\n",
    "\n",
    "<b id=\"discrete-random-variable\" style=\"color:blue\">Discrete Random Variable</b>: A random variable is called a discrete random variable if it has a countable range.\n",
    "\n",
    "<b id=\"continuous-random-variable\" style=\"color:blue\">Continuous Random Variable</b>: A random variable is called a continuous random variable if it has an uncountable range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"events-in-terms-of-random-variables\" style=\"color:blue\">Events in Terms of Random Variables</b>: For a random variable $X$, a real number $x$, and a set $B\\subseteq\\mathbb{R}$, we write\n",
    "\n",
    "$$\\{X=x\\}:=\\{\\omega\\in\\Omega:\\ X(\\omega)=x\\}$$\n",
    "\n",
    "$$\\{X\\in B\\}:=\\{\\omega\\in\\Omega:\\ X(\\omega)\\in B\\}$$\n",
    "\n",
    "and when we have a probability $P$ on $\\Omega$, we further abbreviate:\n",
    "\n",
    "$$P(X=x):=P(\\{X=x\\}),\\qquad P(X\\in B):=P(\\{X\\in B\\})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"probability-mass-function\" style=\"color:blue\">Probability Mass Function</b>: Let $X$ be a discrete random variable with range $R_x$. The function\n",
    "\n",
    "$$f_x(x)=P(X=x)\\ \\text{for each}\\ x\\in R_x$$\n",
    "\n",
    "is called the <b>probability mass function (PMF)</b> of $X$.\n",
    "\n",
    " - $0\\le P(X=x)\\le1$ for all $x$;\n",
    "\n",
    " - $\\sum_{x\\in R_x}P(X=x)=1$;\n",
    "    \n",
    " - for any set $B\\subseteq R_x$, $P(X\\in B)=\\sum_{x\\in B}P(X=x)$.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"function-of-a-random-variable\" style=\"color:blue\">Function of a Random Variable</b>: If $W$ is a random variable and $X=g(W)$, then $X$ is also a random variable where\n",
    "\n",
    "$$R_x=\\{g(w)|w\\in R_w\\}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\n",
    "f_x(x)=\\underset{w:g(x)=x}{\\sum}f_w(w)\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"joint-pmf\" style=\"color:blue\">Joint PMF</b>: The joint probability mass function of two discrete random variables $X$ and $Y$ is defined as\n",
    "\n",
    "$$f_{XY}(x,y)=P(X=x,Y=y)=P(x,y)$$\n",
    "\n",
    "with $P(x,y)\\ge0$ and $\\underset{(x,y)}{\\sum}P(x,y)=1$.\n",
    "\n",
    "The <b id=\"joint-range\" style=\"color:blue\">Joint Range</b> for $X$ and $Y$ is given by\n",
    "\n",
    "$$R_{XY}=\\{(x,y)|P(X=x,Y=y)>0\\}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"marginal-probabilities\" style=\"color:blue\">Marginal Probabilities</b>: If we know the joint PMF/distribution of two random variables $X$ and $Y$, we can also obtain the individual PMF/distribution of $X$ (resp. $Y$):\n",
    "\n",
    "$$P(X=x)=\\underset{y_j\\in R_Y}{\\sum}P(X=x,Y=y_j),\\ \\text{for any}\\ x\\in R_x$$\n",
    "\n",
    "$$P(Y=y)=\\underset{x_i\\in R_X}{\\sum}P(X=x_i,Y=y),\\ \\text{for any}\\ y\\in R_Y$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"identical-distribution\" style=\"color:blue\">Identical Distribution</b>: Random variables have the same or identical distribution if $X$ and $Y$ have the same range, and for every value $v$ in this range,\n",
    "\n",
    "$$P(X=v)=P(Y=v)$$\n",
    "\n",
    "<b id=\"change-of-variable\" style=\"color:blue\">Change of Variable Principle</b>: If $X$ has the same distribution as $Y$, then any statement about $X$ has the same probability as the corresponding statement about $Y$, and $g(X)$ has the distribution as $g(Y)$, for any function $g$.\n",
    "\n",
    "<b id=\"equal-random-variables\" style=\"color:blue\">Equality of Random Variables</b>: Random variables $X$ and $Y$ are equal, written $X=Y$, if $P(X=Y)=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"probabilities-of-events-determined-by-two-random-variables\" style=\"color:blue\">Probabilities of Events Determined by Two Random Variables</b>: The probability that $X$ and $Y$ satisfy some conditions is the sum of $P(X=x, Y=y)=P(x,y)$ over all pairs $(x,y)$ satisfying that condition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"functions-of-two-random-variables\" style=\"color:blue\">Functions of Two Random Variables</b>: Let $X$, $Y$ be two random variables, and suppose that $Z=g(X,Y)$, where $g:\\mathbb{R}^2\\rightarrow\\mathbb{R}$. Then,\n",
    "\n",
    "$$\n",
    "P(Z=z)=\\underset{(x_i,y_j)\\in A_z}{\\sum}P(X=x_i, Y=y_j)\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$A_z=\\{(x_i,y_j)\\in R_{XY}:g(x_i,y_j)=z\\}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"conditional-distributions\" style=\"color:blue\">Conditional Distributions</b>: For any event $A$ and any random variable $Y$, the collection of conditional probabilities\n",
    "\n",
    "$$P(Y\\in B|A)=\\frac{P[(Y\\in B)\\cap A]}{P(A)}$$\n",
    "\n",
    "defines the probability distribution as $B$ varies over subsets of the range of $Y$. This distribution is called the conditional distribution of $Y$ given $A$. \n",
    "\n",
    "If $Y$ is discrete, the conditional distribution of $Y$ given $A$ is specified by the conditional probabilities\n",
    "\n",
    "$$P(Y=y|A)\\quad\\text{for}\\ y\\in R_Y$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b id=\"conditional-pmf\" style=\"color:blue\">\n",
    "\n",
    "Conditional PMF of $Y$ given $A$\n",
    "\n",
    "</b>:For a discrete random variable $Y$ and event $A$, the conditional PMF of $Y$ given $A$ is defined as \n",
    "\n",
    "$$P(Y=y|A)=\\frac{P[(Y=y)\\cap A]}{P(A)}\\qquad\\text{for any }y\\in R_Y$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Difference between this and above??</h1>\n",
    "<b id=\"conditional-pmf-x-and-y\" style=\"color:blue\">\n",
    "\n",
    "Conditional PMF of $X$ given $Y$ and vice versa\n",
    "\n",
    "</b>: For discrete random variables $X$ and $Y$, the conditional PMFs of $X$ given $Y$ and vice versa are defined as \n",
    "\n",
    "$$P(X=x_i|Y=y_j)=\\frac{P(X=x_i,Y=y_j)}{P(X=x_i)}$$\n",
    "\n",
    "$$P(Y=y_j|X=x_i)=\\frac{P(Y=y_i,X=x_j)}{P(Y=y_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"multiplication-rule\" style=\"color:blue\">Multiplication Rule</b>: If the marginal (unconditional) distribution of $X$ is known, together with the conditional distribution of $Y$ given $X=x$ for all possible values $x$ of $X$, the joint distribution of $X$ and $Y$ is found using the multiplication rule\n",
    "\n",
    "$$P(X=x,Y=y)=P(X=x)P(Y=y,X=x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"independent-random-variables\" style=\"color:blue\">Independent Random Variables</b>: Two random variables $X$ and $Y$ are independent if and only if for all intervals $A$, $B\\subseteq\\mathbb{R}$,\n",
    "\n",
    "$$P(X\\in A,Y\\in B)=P(X\\in A)P(Y\\in B)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"theorem-independent-random-variables\" style=\"color:red\">Theorem for Independent Random Variables</b>: If $X$ and $Y$ are discrete random variables, they are independent iff \n",
    "\n",
    "$$P(X=x,Y=y)=P(X=x)P(Y=y)\\quad\\text{for all}\\ x,y\\in\\mathbb{R}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>???</h1>\n",
    "\n",
    "<h4 id=\"several-random-variables\" style=\"color:blue\">Several Random Variables</h4>\n",
    "\n",
    "Let $X_1,\\ldots,X_n$ be discrete random variables.\n",
    "\n",
    " - Joint PMF: $P(X_1=x_1,\\ldots,X_n=x_n)$\n",
    " - Independence: The following three conditions are equivalent\n",
    "    - $X_1,X_2,\\ldots,X_n$ are independent.\n",
    "    - For all intervals $A_1,A_2,\\ldots,A_n\\subseteq\\mathbb{R}$\n",
    "$$P(X_1\\in A_1,\\ldots,X_n\\in A_n)=P(X_1\\in A_1)\\ldots P(X_n\\in A_n)$$\n",
    "    - We have\n",
    "$$P(X_1=x_1,\\ldots,X_n=x_n)=P(X_1=x_1)\\ldots P(X_n=x_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>???</h1>\n",
    "\n",
    "<h4 id=\"independence-several-random-variables\" style=\"color:blue\">Independence of Several Random Variables</h4>\n",
    "\n",
    " - Functions of independent random variables are independent.\n",
    " - Disjoint blocks of independent random variables are independent.\n",
    " - Functions of disjoint blocks of independent random variables are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Stopped at slide 32 lecture 11-12</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"expectation\" style=\"color:blue\">Expectation</b>: The expectation (also called the expected value or mean) of a random variable $X$, is the mean of the distribution of $X$, denoted $E(X)$. That is,\n",
    "\n",
    "$$E(X)=\\underset{x\\in R_X}{\\sum}x\\cdot P(X=x)$$\n",
    "\n",
    "the average of all possible values of $X$, weighted by their probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"mean-median-mode\" style=\"color:blue\">Mean, Median, and Mode</b>: The mean is the average of all possible values of $X$ weighted by their probabilities. The mode is the most likely value of $X$ (there may be more than one). The median is a number $m$ such that both $P(X\\le m)$ and $P(X\\ge m)$ are at least $\\frac{1}{2}$ (there may be more than one).\n",
    "\n",
    "Note: If the distribution of $X$ is symmetric about some point $m$, and has a single mode, the three quantities are equal to $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"expectation-function\" style=\"color:blue\">\n",
    "\n",
    "Expectation of a Function of $X$</b>: Typically, $E[g(x)]\\ne g[E(X)]$. Rather\n",
    "\n",
    "$$E[g(X)]=\\underset{x\\in R_x}{\\sum}g(x)P(X=x)$$\n",
    "\n",
    "<b id=\"expectation-function\" style=\"color:blue\">\n",
    "\n",
    "$k$-th Moment of $X$</b>: For $g(x)=x^k$ with $k=1,2,\\ldots$, the number\n",
    "\n",
    "$$E(X^k)=\\underset{x\\in R_X}{\\sum}x^kP(X=x)$$\n",
    "\n",
    "derived from the distribution of $X$ is called the $k$-th moment of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"properties-of-expectation\" style=\"color:red\">Properties of Expectation</b>: If $a$ and $b$ are constants, then\n",
    "\n",
    " - $E(aX+b)=aE(X)+B$\n",
    " - $E(aX)=aE(X)$\n",
    " - $E(b)=b$\n",
    " - If $X$ and $Y$ are two discrete random variables, then\n",
    "\n",
    "$$E[g(X,Y)]=\\underset{(x,y)\\in R_{XY}}{\\sum}g(x,y)\\cdot P(X=x,Y=y)$$\n",
    "\n",
    " - If $X$ and $Y$ are independent, then\n",
    "\n",
    "$$E(XY)=E(X)E(Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"addition-rule-expectation\" style=\"color:red\">Addition Rule for Expectation</b>: If $X$ and $Y$ are jointly distributed random variables such that $E(X)<\\infty$ and $E(Y)<\\infty$, then\n",
    "\n",
    "$$E(X+Y)=E(X)+E(Y)$$\n",
    "\n",
    "no matter whether $X$ and $Y$ are independent or not. Consequently, for a sequence of random variables $X_1,\\ldots,X_n$ such that $E(X_i)<\\infty$ for $i=1,\\ldots,n$\n",
    "\n",
    "$$E(X_1+\\ldots+X_n)=E(X_1)+\\ldots+E(X_n)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"indicators\" style=\"color:blue\">Indicator Random Variables</b>: If $A$ is an event, the indicator of $A$ is the random variable $I_A$ such that \n",
    "\n",
    "$$I_A(\\omega)=\\begin{cases}1&\\omega\\in A\\\\0&\\omega\\in A^C\\end{cases}$$\n",
    "\n",
    "Sometimes the notation $\\mathbb{1}_A$ is used to remind us that the variable takes the value $1$ on $A$, and $0$ otherwise.\n",
    "\n",
    "$$E(I_A)=1\\cdot P(A)+0\\cdot(1-P(A))=P(A)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"method-of-indicators\" style=\"color:red\">Method of Indicators</b>: Simplifies calculations of the expectation of a counting variable $X$ that counts the number of events that occur in some list of $n$ events, say $A_1,\\ldots,A_n$.\n",
    "\n",
    " - Write $X$ as a sum of indicators,\n",
    "\n",
    "$$X=I_1+\\ldots+I_n$$\n",
    "\n",
    "where $I_j$ is the indicator of $A_j$.\n",
    "\n",
    " - Calculate the expectation of $X$ as\n",
    "\n",
    "$$E(X)=E(I_1+\\ldots+I_n)$$\n",
    "\n",
    "$$E(I_1)+\\ldots+E(I_n)$$\n",
    "\n",
    "$$P(A_1)+\\ldots+P(A_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"tail-sum\" style=\"color:red\">Tail Sum Formula for the Expectation</b>: For $X$ with possible values $\\{0,1,\\ldots,n\\}$:\n",
    "\n",
    "$$E(X)=\\underset{j=1}{\\overset{n}{\\sum}}P(X\\ge j)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"markovs-inequality\" style=\"color:red\">Markov's Inequality</b>: If $X\\ge0$, then\n",
    "\n",
    "$$P(X\\ge a)\\le\\frac{E(X)}{a}$$\n",
    "\n",
    "for every $a>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"variance-standard-deviation\" style=\"color:blue\">Varaince and Standard Deviation</b>: The variance of $X$, denoted $Var(X)$, is the mean squared deviation of $X$ from its expected value $\\mu=E(X)$;\n",
    "\n",
    "$$Var(X)=E[(X-\\mu)^2]$$\n",
    "\n",
    "The standard deviation of $X$, denoted $SD(X)$, is the square root of the variance of $X$:\n",
    "\n",
    "$$SD(X)=\\sqrt{Var(X)}$$\n",
    "\n",
    " - Note that we consider the mean of the squared deviation of $X$ from $\\mu$ since $E[X-\\mu]=E[X]-\\mu=\\mu-\\mu=0$.\n",
    " - $Var(X)=E(X^2)-[E(X)]^2=\\underset{x\\in R_X}{\\sum}x^2P(X=x)-\\left[\\underset{x\\in R_X}{\\sum}xP(X=x)\\right]^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"scaling-shifting\" style=\"color:red\">Scaling and Shifting</b>: For a random variable $X$ and real numbers $a$ and $b$,\n",
    "\n",
    "$$Var(aX+b)=a^2Var(X)$$\n",
    "\n",
    "$$SD(aX+b)=|a|SD(X)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"standardization\" style=\"color:blue\">Standardization</b>: If a random variable $X$ has $E(X)=\\mu$ and $SD(X)=\\sigma>0$, the random variable\n",
    "\n",
    "$$X^*=\\frac{X-\\mu}{\\sigma}$$\n",
    "\n",
    "is called $X$ in standard units or standardization of $X$ has $E(X^*)=0$ and $SD(X^*)=1$.\n",
    "\n",
    " - Positive values of $X^*$ correspond to higher than expected values of $X$; negative values of $X^*$ correspond to lower than expected values of $X$.\n",
    " - Any event determined by the value of $X$ can be rewritten in terms of $X^*$\n",
    "\n",
    " $$P(X\\le b)=P(\\frac{X-\\mu}{\\sigma}\\le\\frac{b-\\mu}{\\sigma})=P(X^*\\le\\frac{b-\\mu}{\\sigma})$$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"chebychevs-inequality\" style=\"color:red\">Chebychev's Inequality</b>: For any random variable $X$ with $E(X)=\\mu$ and $SD(X)=\\sigma$, and any $k>0$,\n",
    "\n",
    "$$P(|X-\\mu|\\ge k\\sigma)\\le\\frac{1}{k^2}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"addition-rule-variances\" style=\"color:red\">Addition Rule for Variances</b>\n",
    "\n",
    " - $Var(X+Y)=Var(X)+Var(Y)$ if $X$ and $Y$ are independent.\n",
    " - $Var(X_1+\\ldots+X_n)=Var(X_1)+\\ldots+Var(X_n)$ if $X_1,\\ldots,X_n$ are independent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"square-root-law\" style=\"color:red\">Square Root Law</b>: Let $S_n$ be the sum, $\\bar{X}_n=\\frac{S_n}{n}$ the average, of $n$ independent random variables $X_1,\\ldots,X_n$ each with the same distribution as $X$. Then,\n",
    "\n",
    "$$E(S_n)=nE(X)\\qquad\\text{and}\\qquad SD(S_n)=\\sqrt{n}SD(X)$$\n",
    "\n",
    "$$E(\\bar{X}_n)=E(X)\\qquad\\text{and}\\qquad SD(\\bar{X}_n)=\\frac{SD(X)}{\\sqrt{n}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"law-of-averages\" style=\"color:red\">Law of Averages/Weak Law of Large Numbers</b>: Let $X_1,X_2,\\ldots$ be a sequence of independent random variables with the same distribution as $X$. Let $\\mu=E(X)$ denote the common expected value of $X_i$, and let\n",
    "\n",
    "$$\\bar{X}_n=\\frac{X_1+X_2+\\ldots+X_n}{n}$$\n",
    "\n",
    "be the random variable representing the average of $X_1,\\ldots,X_n$. THen for every $\\epsilon>0$, no matter how small,\n",
    "\n",
    "$$P(|\\bar{X}_n-\\mu|<\\epsilon)\\rightarrow1\\quad\\text{as}\\ n\\rightarrow\\infty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"central-limit-theorem\" style=\"color:red\">Normal Approximation (Central Limit Theorem)</b>: Let $S_n=X_1+\\ldots+X_n$ be the sum of $n$ independent random variables each with the same distribution over some finite set of values. For large $n$, the distribution of $S_n$ is approximately normal, with mean $E(S_n)=n\\mu$, and standard deviation $SD(S_n)=\\sigma\\sqrt{n}$, where $\\mu=E(X_i)$ and $\\sigma=SD(X_i)$. For all $a\\le b$\n",
    "\n",
    "$$P(a\\le\\frac{S_n-n\\mu}{\\sigma\\sqrt{n}}\\le b)\\approx\\Phi(b)-\\Phi(a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"discrete-random-variable\" style=\"color:blue\">Discrete Random Variable</b>: A random variable is called a discrete random variable if it has a countable range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"discrete-distributions-nonnegative-integers\" style=\"color:blue\">Discrete Distributions on the Non-negative Integers</b>: A discrete distribution on the set of non-negative integers $\\{0,1,2,\\ldots\\}$ is defined by a sequence of probabilities $p_0,p_1,p_2,\\ldots$ such that\n",
    "\n",
    "$$p_i\\ge0\\ \\text{for all}\\ i\\ \\text{and}\\ \\underset{i=0}{\\overset{\\infty}{\\sum}}p_i=1$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"infinite-sum-rule\" style=\"color:red\">Infinite Sum Rule</b>: If event $A$ is partitioned into $A_1,A_2,A_3,\\ldots,$ i.e.,\n",
    "\n",
    "$$A=A_1\\cup A_2\\cup A_3\\cup\\ldots\\quad\\text{where}\\quad A_i\\cap A_j=\\emptyset\\ (i\\ne j)$$\n",
    "\n",
    "then\n",
    "\n",
    "$$P(A)=P(A_1)+P(A_2)+P(A_3)+\\ldots=\\underset{i=1}{\\overset{\\infty}{\\sum}}P(A_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"expectation-discrete-random-variable\" style=\"color:blue\">Expectation of a Discrete Random Variable</b>: The expectation fo a discrete random variable $X$ is defined by\n",
    "\n",
    "$$E(X)=\\underset{x}{\\sum}xP(X=x)$$\n",
    "\n",
    "provided that the series is absolutely convergent, i.e., provided\n",
    "\n",
    "$$\\underset{x}{\\sum}|x|P(X=x)<\\infty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"geometric-distribution\" style=\"color:blue\">Geometric Distribution</b>: Let $X$ be the waiting time until the first success in a sequence of independent $Bernoulli(p)$ trials. Then, $X\\sim Geo(p)$\n",
    "\n",
    "$$P(X=x)=p(1-p)^{x-1},\\quad x=1,2,3,\\ldots$$\n",
    "\n",
    "$$E(X)=\\frac{1}{p}\\ \\text{and}\\ Var(X)=\\frac{1-p}{p^2}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"negative-binomial-distribution\" style=\"color:blue\">Negative Binomial Distribution</b>: Let $X$ denote the waiting time/number of trials until the $r$-th success in a sequence of independent $Bernoulli(p)$ trials. Then, $X\\sim NegBin(r, p)$\n",
    "\n",
    "$$P(X=x)=\\begin{pmatrix}x-1\\\\r-1\\end{pmatrix}p^r(1-p)^{x-r}\\quad\\text{for}\\ x=r,r+1,r+2,\\ldots$$\n",
    "\n",
    "$$E(X)=\\frac{r}{p}\\ \\text{and}\\ Var(X)=\\frac{r(1-p)}{p^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"poisson-random-variable\" style=\"color:blue\">Poisson Random Variable</b>: A random variable $X$ is said to be a Poisson random variable with parameter $\\mu$, denoted $X\\sim Poisson(\\mu)$, if its range $R_X=\\{0,1,2,\\ldots\\}$, and its PMF is given by\n",
    "\n",
    "$$P(X=k)=\\begin{cases}\\frac{e^{-\\mu}\\mu^k}{k!}&k\\in R_X\\\\0&\\text{else}\\end{cases}$$\n",
    "\n",
    "$$E(X)=\\mu\\ \\text{and}\\ Var(X)=\\mu$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"sum-poisson-random-variables\" style=\"color:red\">Sum of Independent Poisson Random Variables</b>: If $X_1,\\ldots,X_n$ are independent Poisson random variables with parameter $\\mu_1,\\ldots,\\mu_n$, then $X_1+\\ldots+X_n$ is a Poisson random variable with parameter $\\mu_1+\\ldots+\\mu_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"continuous-random-variables\" style=\"color:blue\">Continuous Random Variables</b>: A random variable is called a continuous random variable if it has an uncountable range (such as an interval on the real line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"probability-density-functions\" style=\"color:blue\">Probability Density Functions</b>: A function $f_x:\\mathbb{R}\\rightarrow\\mathbb{R}$ is a probability density function for a continuous random variable $X$ iff\n",
    "\n",
    "$$P(a\\le X\\le b)=\\in_a^bf_X(x)dx\\quad\\text{for all}\\ a\\le b$$\n",
    "\n",
    " - $f(x)\\ge0$ for all $x\\in\\mathbb{R}$, and\n",
    "\n",
    " - $\\int_{-\\infty}^{\\infty}f(x)dx=1$\n",
    "\n",
    " - $P(X=a)=P(a\\le X\\le a)=\\int_a^af_X(x)dx=0$, which indicates that\n",
    "\n",
    " - $P(a\\le X\\le b)=P(a< X\\le b)=P(a\\le X<b)=P(a< X<b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"continuous-expectation-variance\" style=\"color:blue\">Continuous Expectation and Varaince</b>: Let $X$ be a continuous random variable with probability density function $f_x$. \n",
    "\n",
    "$$E(X)=\\int_{-\\infty}^{\\infty}x\\cdot f_x(x)dx\\quad \\text{and}\\quad E[g(X)]=\\int_{-\\infty}^{\\infty}g(x)\\cdot f_x(x)dx$$\n",
    "\n",
    "$$Var(X)=\\int_{-\\infty}^{\\infty}x^2f_x(x)dx-\\mu^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"uniform-distribution\" style=\"color:blue\">Uniform Distribution</b>: A continuous random variable $X$ has uniform distribution over the interval $[a,b]$ denoted $X\\sim Uniform(a,b)$\n",
    "\n",
    "$$f_x(x)=\\begin{cases}\\frac{1}{b-a}&a\\le x\\le b\\\\0&x<a\\ \\text{or}\\ x>b\\end{cases}$$\n",
    "\n",
    "$$E(X)=\\frac{a+b}{2}\\quad\\text{and}\\quad Var(X)=\\frac{(b-a)^2}{12}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"normal-distribution\" style=\"color:blue\">Normal Distribution</b>: The normal distribution with mean $\\mu$ and standard deviation $\\sigma$ is the distiribution over the $x$-axis defined by areas under the normal curve with these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"normal-standard-normal\" style=\"color:blue\">Normal and Standard Normal Distribution</b>: \n",
    "\n",
    "<center>\n",
    "\n",
    "$X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ iff $Z:=\\frac{X-\\mu}{\\sigma}\\sim\\mathcal{N}(0,1)$\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id=\"normal-standard-normal\" style=\"color:blue\">Standard Normal CDF</b>: Let $Z\\sim\\mathcal{N}(0,1)$. The standard normal cumulative distribution function $\\Phi(z)$ gives the area to the left of $z$ under the standard normal curve:\n",
    "\n",
    "$$\\Phi(z)=P(Z\\le z)=\\int_{-\\infty}^z\\phi(y)dy$$\n",
    "\n",
    "Let $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$. Then,\n",
    "\n",
    "$$P(a\\le X\\le b)=\\Phi(\\frac{b-\\mu}{\\sigma})-\\Phi(\\frac{a-\\mu}{\\sigma})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
